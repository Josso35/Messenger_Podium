{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "light-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import fr_core_news_sm\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#nltk.download()\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "close-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_noun=['©',',','?','.','!','ã',' ','\"','','ã§a','\\n','(','etre','avoir','cela','caest','faire','aller',\n",
    "          'tout','bon','plus','bien','ahah','voir','être','si','pouvoir','haha','non',';)','🤣','quand','aussi','quoi',\n",
    "         'bah','c’','trop','dire','venir','oui',')','comme','savoir','^^','petit','falloir','vouloir','@antoine','ah','chez','-',\n",
    "         'prendre','liberg','..','...','hahaha',':)','ha',':','','🐟','moins','pense','passer','encore','n\\'','  ','   ','aa',\n",
    "         'da','de','e','ra','thomas','taire']\n",
    "my_list = stopwords.words('english') + stopwords.words('french') + non_noun\n",
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coding problem from Facebook menssenger side\n",
    "def parse_obj(obj):\n",
    "    for key in obj:\n",
    "        if isinstance(obj[key], str):\n",
    "            obj[key] = obj[key].encode('latin_1').decode('utf-8')\n",
    "        elif isinstance(obj[key], list):\n",
    "            obj[key] = list(map(lambda x: x if type(x) != str else x.encode('latin_1').decode('utf-8'), obj[key]))\n",
    "        pass\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "def load_all_messages(path):\n",
    "    # Open first the first message\n",
    "    file = open(path + 'message_1.json')\n",
    "    \n",
    "    #Here we have the decoder from messnenger\n",
    "    data = json.load(file, object_hook=parse_obj)\n",
    "    \n",
    "    #\n",
    "    df = pd.json_normalize(data['messages'])\n",
    "    \n",
    "    #Then open the other ones and append them\n",
    "    #Would need to change that to apply to every number of files needed\n",
    "    for i in np.arange(2,6) : \n",
    "        file = open(path + 'message_'+str(i)+'.json', encoding='utf8')\n",
    "        data = json.load(file, object_hook=parse_obj)\n",
    "        df_temp = pd.json_normalize(data['messages'])\n",
    "        df=df.append(df_temp)\n",
    "    return (df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disturbed-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #We want a usable time stamp\n",
    "    df['date_time']=pd.to_datetime(df['timestamp_ms'], unit='ms') \n",
    "    \n",
    "    #Way easier to work with lower cases for text\n",
    "    df['content']=df['content'].str.lower()\n",
    "    \n",
    "    #Let's not work first with every data --> Only text\n",
    "    df.drop(columns=['timestamp_ms','gifs','is_unsent','photos','type','videos','audio_files','sticker.uri',\n",
    "                     'call_duration','share.link','share.share_text','users','files'],inplace=True)\n",
    "\n",
    "    df['year']=df['date_time'].dt.year\n",
    "    #df=df[df['year']==2021]\n",
    "    df['hour']=df['date_time'].dt.hour\n",
    "    df['weekday']=df['date_time'].dt.weekday\n",
    "    \n",
    "    #We can exclude some non participing people\n",
    "    df=df[~df['sender_name'].isin([''])]\n",
    "    \n",
    "    df['content']=df.content.fillna('')\n",
    "    #df=df[~df['content'].isna()].reset_index()\n",
    "    \n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "special-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lemma ( df): \n",
    "    df['parsed_content'] = df['content'].apply(lambda x: [y.lemma_ for y in  nlp(x)])\n",
    "    temmenized=df.explode('parsed_content')[['sender_name','parsed_content']]\n",
    "    temmenized['parsed_content']=temmenized['parsed_content'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    \n",
    "    \n",
    "    temmenized['nb_use']=1\n",
    "    return(temmenized)\n",
    "\n",
    "def clean_token( df) :\n",
    "    #because the lemmatization is long let's update easier the stop word list\n",
    "    df=df[~(df['parsed_content'].isin(final_stopwords_list_1+my_list))]\n",
    "    return (df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "resistant-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_information( df): \n",
    "    #number of word per message\n",
    "    df['word_number'] = df['parsed_content'].apply(lambda x: len(x))\n",
    "    \n",
    "    #if the next message same sender =1\n",
    "    df['answered_himself']=df.sender_name.eq(df.sender_name.shift())\n",
    "    \n",
    "    #number of reaction\n",
    "    df['reactions']=df.reactions.fillna('')\n",
    "    df['reaction_received'] = df['reactions'].apply(lambda x: len(x))\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lined-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_sender(df):\n",
    "    df['is_message']=1\n",
    "    df['has_reac']=df['reaction_received'] >0\n",
    "    filter_col = [col for col in df if col.startswith('reac_from_')]\n",
    "\n",
    "    df_grouped=df.groupby('sender_name').agg({'word_number': ['sum', 'max','mean','median'], \n",
    "                                   'answered_himself':['sum','mean'],\n",
    "                                   'reaction_received':['sum','mean'],\n",
    "                                    'has_reac':['sum','mean'],\n",
    "                                    'is_message':['count']})\n",
    "    \n",
    "    \n",
    "    df_grouped_has_reacfrom=df[filter_col+['sender_name']].groupby('sender_name').sum()\n",
    "    df_grouped=df_grouped.merge(df_grouped_has_reacfrom,left_index=True, right_index=True)\n",
    "    return(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minor-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactions_from(df):\n",
    "    sender_names=df.sender_name.unique()\n",
    "    \n",
    "    df_react=pd.json_normalize(df['reactions'])\n",
    "    nb_col=df_react.shape[1]\n",
    "    \n",
    "    for col_i in np.arange(nb_col):\n",
    "        df_react[col_i]=pd.json_normalize(df_react[col_i])['actor']\n",
    "        \n",
    "    for sender in sender_names:\n",
    "        df['reac_from_'+sender]=(df_react==sender).any(axis=1)\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gross-medline",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2021' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-eded76894eb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday_max_message\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhours\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_max_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mby_sender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_2021\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemmenized\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Thomas Liberge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_2021' is not defined"
     ]
    }
   ],
   "source": [
    "def by_sender(df,temmenized,sender_name):\n",
    "    \n",
    "    if sender_name != 'all' :\n",
    "        df=df[df['sender_name']==sender_name]\n",
    "        temmenized=temmenized[temmenized['sender_name']==sender_name].dropna()\n",
    "        \n",
    "        \n",
    "        \n",
    "    day_max_message=df['date_time'].dt.date.value_counts().head(2)\n",
    "    \n",
    "    hours=df[['content','hour']].groupby(by='hour').count().T/360\n",
    "    \n",
    "    day=df[['content','weekday']].groupby(by='weekday').count().T/50\n",
    "    \n",
    "    word_max_freq=temmenized['parsed_content'].value_counts().head(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(day_max_message,hours,day,word_max_freq)\n",
    "\n",
    "by_sender(df_2021,temmenized,'Thomas Liberge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../0. Data/\"\n",
    "df=load_all_messages(path)\n",
    "df=clean_data(df)\n",
    "temmenized= token_lemma ( df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "temmenized= clean_token ( temmenized)\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "df= add_information( df)\n",
    "df= reactions_from(df)\n",
    "\n",
    "df_2021=df[df['year']==2021].reset_index(drop=True)\n",
    "df_2020=df[df['year']==2020].reset_index(drop=True)\n",
    "\n",
    "df_grouped_2021=groupby_sender(df_2021)\n",
    "df_grouped_2020=groupby_sender(df_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with pd.ExcelWriter(\"../2. Output/Data.xlsx\", engine='openpyxl',mode='w') as writer:\n",
    "    \n",
    "        \n",
    "        df_grouped_2021.to_excel(writer,sheet_name='grouped',startrow=1)\n",
    "        df_grouped_2020.to_excel(writer,sheet_name='grouped',startrow=15)\n",
    "        \n",
    "        sender_list =  np.concatenate((df.sender_name.unique(),['all']))\n",
    "        for sender in sender_list : \n",
    "            print(sender)\n",
    "            day_max_message,hours,day,word_max_freq =  by_sender(df,temmenized,sender)\n",
    "            \n",
    "            day_max_message.to_excel(writer,sheet_name=sender,startrow=0)\n",
    "            hours.to_excel(writer,sheet_name=sender,startrow=5)\n",
    "            day.to_excel(writer,sheet_name=sender,startrow=10)\n",
    "            word_max_freq.to_excel(writer,sheet_name=sender,startrow=15)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-shooting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sender_name.unique().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2=temmenized[temmenized['parsed_content']!='  ']\n",
    "v2['parsed_content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-lounge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
